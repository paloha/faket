{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458f1574-baad-428c-971f-eaeebb29863d",
   "metadata": {},
   "source": [
    "# Reproducing FakET experiments on SHREC21 data\n",
    "\n",
    "Some of the data preparation is done directly in this notebook. E.g. slicing the mrc files, or creating noiseless or noisy projections or running the reconstructions. However, the GPU tasks are only created here to be submitted to SLURM. The jobs can be submitted to SLURM directly from this notebook if the setup is appropriate. Therefore, it would be ideal if in your setup, this notebook was running in the appropriate conda environment from where it can execute tasks on multiple CPUs and at the same time submit GPU or CPU jobs to SLURM using sbatch command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278fc9a0-1ecf-42bc-b4ab-8547612cd82e",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **WARNING:** **Naming convention**\n",
    ">\n",
    "> Before the article submission we changed the naming of the methods to make it easier to understand our experiments and aim.\n",
    "In this notebook, we still use the old naming. More precisely the old naming maps to new according to the following map:\n",
    "> ```   \n",
    ">naming = {\n",
    ">    'baseline': 'BENCHMARK',\n",
    ">    'gauss': 'BASELINE',\n",
    ">    'styled': 'FAKET',\n",
    ">    'noisy': 'NOISY',\n",
    ">}\n",
    ">```\n",
    "\n",
    "> **TIP:** **Reproducing the paper**\n",
    ">\n",
    "> If you just want to run the experiments we did, everytime you come across cells that generate sbatch files\n",
    "you do not actually have to re-create them. Just use the sbatch files we provided in the repo. However, it\n",
    "is highly-likely your SLURM config is different than ours and you will need to re-create the files anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed96c33-8e0d-40b5-9834-d7955a6320cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087ac43-176d-40f4-b431-0d5011467967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import gpuMultiprocessing\n",
    "from itertools import product\n",
    "from os.path import join as pj\n",
    "import matplotlib.pyplot as plt\n",
    "from faket.data import vol_to_valid, get_theta\n",
    "from faket.data import load_mrc, save_mrc, save_conf\n",
    "from faket.noisy import estimate_noise_curves\n",
    "from faket.noisy import aggregate, noise_projections\n",
    "from faket.transform import radon_3d, reconstruct_mrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba7269-c7a8-4ada-8cde-2a4893c101a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data should be stored in the following folder\n",
    "data_folder = 'data/shrec2021_extended_dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83450709-63c0-459d-ad7d-98e606b9bfe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Slicing volumes to valid voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63139552-e378-494b-ad7a-aff7fd6f72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHREC21 provides the data in square shape even\n",
    "# thought the data is stored only in the center\n",
    "# The following values specify where to slice\n",
    "z_valid = (0.32226, 0.67382)  # Valid range normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b557c4-bd47-44d5-80b0-f1e5151c32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice class_mask.mrc to faket/class_mask.mrc\n",
    "for N in range(10):\n",
    "    vol_to_valid(data_folder, f'model_{N}', 'class_mask', z_valid, \n",
    "                 out_fname='faket/class_mask.mrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b8d9-1636-4ddd-853f-f34ff7012ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice occupancy_mask.mrc to faket/occupancy_mask.mrc\n",
    "for N in range(10):\n",
    "    vol_to_valid(data_folder, f'model_{N}', 'occupancy_mask', z_valid, \n",
    "                 out_fname='faket/occupancy_mask.mrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce1262-c1e2-4cea-b7d2-90d178bb362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice reconstruction.mrc to faket/reconstruction_shrec.mrc\n",
    "for N in range(10):\n",
    "    vol_to_valid(data_folder, f'model_{N}', 'reconstruction', z_valid, \n",
    "                 out_fname='faket/reconstruction_shrec.mrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087354b7-0d6d-4a99-8170-3ddb28916046",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating projections from grandmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c95a0-bc1d-4aec-8e56-9c8acc07c81e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noiseless projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdde34-b4f6-4c99-a6b6-53d9b6931c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create faket/projections_noiseless.mrc by measuring the grandmodel_unbinned.mrc with Radon transform\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = {\n",
    "        'input_mrc': pj(data_folder, f'model_{N}', 'grandmodel_unbinned.mrc'),\n",
    "        'output_mrc': pj(data_folder, f'model_{N}', 'faket/projections_noiseless.mrc'),\n",
    "        'radon_kwargs': {\n",
    "            'theta': get_theta(data_folder, N),\n",
    "            'dose': 0,\n",
    "            'out_shape': 1024,\n",
    "            'slice_axis': 1,\n",
    "            # circle=False because we measure with the data outside the circle \n",
    "            # but later we cut the measurements to desired shape \n",
    "            # SHREC did it this way - confirmed from a personal communication\n",
    "            'circle': False\n",
    "        }\n",
    "    }\n",
    "    volume = load_mrc(conf['input_mrc'])\n",
    "    sinogram = radon_3d(volume, **conf['radon_kwargs'])\n",
    "    save_conf(conf['output_mrc'], conf)\n",
    "    save_mrc(sinogram.astype(np.float32), conf['output_mrc'], overwrite=True)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa488b-4dd8-4923-be72-1c1df6e917cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noisy & Content & Gauss projections\n",
    "\n",
    "Adding Gaussian noise in projection space and matching tilt-wise mean&std of style projections.\n",
    "Noise statistics are estimated automatically from style projections based on the whole training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67f8e5-e5a6-47ab-8c50-75817684fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "noisy_conf = {\n",
    "    'noise_estimation_noisy_paths': [pj(data_folder, f'model_{N}', f'projections_unbinned.mrc') for N in range(9)],\n",
    "    'noise_estimation_clean_paths': [pj(data_folder, f'model_{N}', 'faket/projections_noiseless.mrc') for N in range(9)],\n",
    "    'noise_estimation_aggregation_order': 2, \n",
    "    'noise_estimation_aggregation_n': 'all', \n",
    "}\n",
    "\n",
    "print('Estimating noise')\n",
    "# Estimating the noise statistics for each tilt of each training tomogram\n",
    "all_curves = estimate_noise_curves(\n",
    "    projections_noisy_paths=noisy_conf['noise_estimation_noisy_paths'], \n",
    "    projections_clean_paths=noisy_conf['noise_estimation_clean_paths'])\n",
    "\n",
    "# Aggregate stats\n",
    "agg_o = noisy_conf['noise_estimation_aggregation_order']\n",
    "agg_n = noisy_conf['noise_estimation_aggregation_n']\n",
    "aggregated_rs = aggregate([c['rs'] for c in all_curves], order=agg_o, n=agg_n)\n",
    "aggregated_σs = aggregate([c['stds'] for c in all_curves], order=agg_o, n=agg_n)\n",
    "aggregated_style_μs = aggregate([c['noisy_μs'] for c in all_curves], order=agg_o, n=agg_n)\n",
    "aggregated_style_σs = aggregate([c['noisy_σs'] for c in all_curves], order=agg_o, n=agg_n)\n",
    "\n",
    "# Aggregate stats for Gauss modality (best possible guess if setting sigma of global gaussian noise manually)\n",
    "# This is supposed to mimic what people in CryoEM do nowadays when generating synthetic data (assuming they\n",
    "# add the noise in the projection space, which is not necessarily true as some might go easier way and just\n",
    "# add the noise in the reconstruction space.\n",
    "global_aggregated_rs = aggregate([c['rs'] for c in all_curves], order=0, n=agg_n)\n",
    "global_aggregated_σs = aggregate([c['stds'] for c in all_curves], order=0, n=agg_n)\n",
    "global_aggregated_style_μs = aggregate([c['noisy_μs'] for c in all_curves], order=0, n=agg_n)\n",
    "global_aggregated_style_σs = aggregate([c['noisy_σs'] for c in all_curves], order=0, n=agg_n)\n",
    "\n",
    "print('Noise estimated.\\n')\n",
    "\n",
    "# create faket/projections_content.mrc and faket/projections_noisy.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    \n",
    "    # Noisy modality\n",
    "    conf = {\n",
    "        'input_mrc': pj(data_folder, f'model_{N}', 'faket/projections_noiseless.mrc'), \n",
    "        'output_mrc': pj(data_folder, f'model_{N}', 'faket/projections_noisy.mrc'),\n",
    "        'r': list(aggregated_rs),\n",
    "        'std': list(aggregated_σs),\n",
    "        'style_mrc': None,  # Instead of one style_mrc we use aggregated stats from training set\n",
    "        'style_means': list(aggregated_style_μs),\n",
    "        'style_stds': list(aggregated_style_σs),\n",
    "        'seed': N,\n",
    "    }\n",
    "    save_conf(conf['output_mrc'], dict(noisy_conf, **conf))\n",
    "    noise_projections(**conf)\n",
    "    \n",
    "    # Content modality\n",
    "    conf.update({\n",
    "        'std': list(aggregated_σs * 0.25),\n",
    "        'output_mrc': pj(data_folder, f'model_{N}', 'faket/projections_content.mrc'),\n",
    "    })\n",
    "    save_conf(conf['output_mrc'], dict(noisy_conf, **conf))\n",
    "    noise_projections(**conf)\n",
    "    \n",
    "    # Gauss modality\n",
    "    conf.update({\n",
    "        'r': list(global_aggregated_rs),\n",
    "        'std': list(global_aggregated_σs),\n",
    "        'style_means': list(global_aggregated_style_μs),\n",
    "        'style_stds': list(global_aggregated_style_σs),\n",
    "        'output_mrc': pj(data_folder, f'model_{N}', 'faket/projections_gauss.mrc'),\n",
    "    })\n",
    "    gauss_conf = dict(noisy_conf, **conf)\n",
    "    gauss_conf['noise_estimation_aggregation_order'] = 0\n",
    "    save_conf(conf['output_mrc'], gauss_conf)\n",
    "    noise_projections(**conf)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b64d09-428b-4d53-8182-f0e5d3992e63",
   "metadata": {},
   "source": [
    "To get just plain addition of gaussian noise with one std do what is above just with order = 0.\\\n",
    "Summary of what is happening is bellow:\n",
    "\n",
    "```\n",
    "one_r = aggregate([c['rs'] for c in all_curves], order=0, n='all')[0]\n",
    "one_σ = aggregate([c['stds'] for c in all_curves], order=0, n='all')[0]\n",
    "\n",
    "x = standardize_per_tilt(projections_noiseless) * one_r\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "x += (rng.normal(loc=0, scale=1, size=np.prod(x.shape)).reshape(x.shape) * one_σ)\n",
    "\n",
    "# And finally match x per tilt with mean and std of style or provided arrays\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd41c-45d9-426d-b1da-484bbff83ef1",
   "metadata": {
    "tags": []
   },
   "source": [
    ":TIP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248df2ef-3676-4052-86a6-0947bf7b0ca5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Styled projections\n",
    "\n",
    "Neural Style Transfer in projection space (needs GPU in order to be reasonably fast).\\\n",
    "On CPU, to do one NST on 61x1024x1024 sinogram takes about 5 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927792d-dd97-409b-a1c6-67e7952f1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstc = {  # NEURAL STYLE TRANSFER BASE CONFIG\n",
    "    # The commented params will be set later\n",
    "    # 'content': 'example.mrc',\n",
    "    # 'style': 'example.mrc',\n",
    "    # '--init': 'example.mrc',\n",
    "    # '--output': 'example.mrc', \n",
    "    # '--random-seed': None,\n",
    "    '--style-weights': 1.0,  # if number of style images is 1, 1.0 is the same as None\n",
    "    '--content-weight': 1.0,  # weight of the content loss relative to style loss\n",
    "    '--tv-weight': 0,  # No Total Variation is desired here\n",
    "    '--min-scale': 1024,\n",
    "    '--end-scale': 1024,\n",
    "    '--iterations': 1,\n",
    "    '--initial-iterations': 1,\n",
    "    '--save-every': 2,\n",
    "    '--step-size': 0.15,\n",
    "    '--avg-decay': 0.99,\n",
    "    '--style-scale-fac': 1.0,\n",
    "    '--pooling': 'max',\n",
    "    '--devices': 'cuda:0',\n",
    "    '--seq_start' : 0,\n",
    "    '--seq_end' : 61,\n",
    "    '--content_layers': '8',\n",
    "    '--content_layers_weights': '100',\n",
    "    '--model_weights': 'pretrained'\n",
    "}\n",
    "\n",
    "# to run on cpu: f\"srun --qos=cpusonly --cpus-per-task=1 --ntasks=1 --export=ALL \"\n",
    "def get_command(expname, nst_command, config):\n",
    "    command = (\n",
    "    \"#!/bin/bash\\n\"\n",
    "    f\"#SBATCH --job-name {expname}\\n\"\n",
    "    \"#SBATCH --qos=gpus2\\n\"\n",
    "    \"#SBATCH --gres=gpu:1\\n\"\n",
    "    \"#SBATCH --cpus-per-task=5\\n\"\n",
    "    \"#SBATCH --nodes 1\\n\"\n",
    "    \"#SBATCH --ntasks=1\\n\"\n",
    "    \"#SBATCH --output reproduce/nst/%x.out\\n\\n\"\n",
    "    f\"EXPNAME={expname} PYTHONHASHSEED=0 \"\n",
    "    f\"{nst_command} {config['content']} {config['style']} \"\n",
    "    f\"{' '.join([f'{k} {v}' for k, v in config.items() if k.startswith('--')])}\")\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94bfe9-cecf-4260-b185-843fa356d199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create faket/projections_styled.mrc\n",
    "# will create number of sbatch files in the selected folder\n",
    "NST_command = 'python3 -m faket.style_transfer.cli'\n",
    "folder = 'reproduce/nst'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "jobs = []\n",
    "for N in range(0, 10):\n",
    "    style_N = (N + 1) % 9 # For the last train model we take style stats from the first train model\n",
    "    \n",
    "    EXPNAME = f'nst_tomogram_{N}'  # Just for visualizing the progress\n",
    "    tomo_folder = pj(data_folder, f'model_{N}', 'faket')\n",
    "\n",
    "    conf = nstc.copy()\n",
    "    conf.update({\n",
    "        'content': pj(tomo_folder, 'projections_content.mrc'),\n",
    "        'style': pj(data_folder, f'model_{style_N}', 'projections_unbinned.mrc'), \n",
    "        '--init': pj(tomo_folder, 'projections_noisy.mrc'),\n",
    "        '--output': pj(tomo_folder, 'projections_styled.mrc'), \n",
    "        '--random-seed': N,\n",
    "    })\n",
    "    \n",
    "    command = get_command(EXPNAME, NST_command, conf)\n",
    "    job_path = pj(folder, f'{EXPNAME}.sbatch')\n",
    "    with open(job_path, 'w') as fl:\n",
    "        fl.writelines(command)  \n",
    "    jobs.append(job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d1c48-d1f1-4f45-be6d-89becce0718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs to the SLURM queue\n",
    "for job_path in jobs:\n",
    "    !sbatch $job_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808e94a-4e7b-439c-ad2b-c0fac8ed86be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computing reconstructions\n",
    "\n",
    "The reconstructions run directly from this notebook (i.e. no sbatch jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63637366-ba08-408e-8dc2-473fd13a328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recc = {  # RECONSTRUCTION BASE CONFIG\n",
    "    'downsample_angle' : 1,  # Sinogram downsampling in theta dimension (1 = no downsampling)\n",
    "    'downsample_pre' : 2,  # Sinogram downsampling (1 = no downsampling)\n",
    "    'order' : 3,  # Downsampling in space with spline interpolation of order (0 - 5)\n",
    "    'filtering' : 'approxShrec',  # Filter used during reconstruction in FBP algorithm\n",
    "    'filterkwargs' : None,\n",
    "    'downsample_post' : 1,  # Reconstruction downsampling\n",
    "    'z_valid': z_valid, # 2-tuple range of valid pixels in Z dimension normalized from 0 to 1. (0., 1.) or None for all.\n",
    "    'software': 'radontea',  # or imod\n",
    "    'ncpus': 61, # multiprocessing.cpu_count(),  # Number of CPUs to use while reconstructing with radontea\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797c6ea-1299-4e6b-b135-75043cf73a5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noiseless reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037af53-80ca-4a9c-a2c2-8682d308454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct faket/projections_noiseless.mrc to produce faket/reconstruction_noiseless.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = recc.copy()\n",
    "    conf.update({\n",
    "        'input_mrc' :  pj(data_folder, f'model_{N}', 'faket/projections_noiseless.mrc'), \n",
    "        'theta': pj(data_folder, f'model_{N}', 'alignment_simulated.txt'), \n",
    "        'output_mrc' : pj(data_folder, f'model_{N}', 'faket/reconstruction_noiseless.mrc')\n",
    "    })\n",
    "    reconstruct_mrc(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8983423-faed-4e3a-9cad-27434d8734e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline reconstructions (in paper referred to as BENCHMARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68c034-b3e5-4f7b-9c3e-567b57e3b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct projections_unbinned.mrc to produce faket/reconstruction_baseline.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = recc.copy()\n",
    "    conf.update({\n",
    "        'fix_edges_proj': 8,  # Fixing artifacts in SHREC projections\n",
    "        'fix_edges_rec': 20,  # Fixing artifacts in SHREC reconstructions\n",
    "        'input_mrc' :  pj(data_folder, f'model_{N}', 'projections_unbinned.mrc'), \n",
    "        'theta': pj(data_folder, f'model_{N}', 'alignment_simulated.txt'), \n",
    "        'output_mrc' : pj(data_folder, f'model_{N}', 'faket/reconstruction_baseline.mrc')\n",
    "    })\n",
    "    reconstruct_mrc(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0a746-77fc-4b29-8cf7-b3298a4c7d88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noisy reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b252a9-1d7f-4357-9e70-2f068a815f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct faket/projections_noisy.mrc to produce faket/reconstruction_noisy.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = recc.copy()\n",
    "    conf.update({\n",
    "        'input_mrc' : pj(data_folder, f'model_{N}', 'faket/projections_noisy.mrc'), \n",
    "        'theta': pj(data_folder, f'model_{N}', 'alignment_simulated.txt'), \n",
    "        'output_mrc' : pj(data_folder, f'model_{N}', 'faket/reconstruction_noisy.mrc')\n",
    "    })\n",
    "    reconstruct_mrc(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1ccc8-a155-4165-91ab-b286543f4e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gauss reconstructions (in paper referred to as BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba319ff-7520-4496-8148-298a43dba793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct faket/projections_noisy.mrc to produce faket/reconstruction_noisy.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = recc.copy()\n",
    "    conf.update({\n",
    "        'input_mrc' : pj(data_folder, f'model_{N}', 'faket/projections_gauss.mrc'), \n",
    "        'theta': pj(data_folder, f'model_{N}', 'alignment_simulated.txt'), \n",
    "        'output_mrc' : pj(data_folder, f'model_{N}', 'faket/reconstruction_gauss.mrc')\n",
    "    })\n",
    "    reconstruct_mrc(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b422aef-9cd0-4a6c-b906-1d0b3cfb49f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Styled reconstructions (in paper referred to as FAKET)\n",
    "This cell has to be run only after all commands from `reproduce/nst` are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c2fbf-abf6-4316-a492-b92601a7c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct faket/projections_styled.mrc to produce faket/reconstruction_styled.mrc\n",
    "for N in range(0, 10):\n",
    "    print(f'Processing N: {N}')\n",
    "    conf = recc.copy()\n",
    "    conf.update({\n",
    "        'input_mrc' : pj(data_folder, f'model_{N}', f'faket/projections_styled.mrc'), \n",
    "        'theta': pj(data_folder, f'model_{N}', 'alignment_simulated.txt'), \n",
    "        'output_mrc' : pj(data_folder, f'model_{N}', f'faket/reconstruction_styled.mrc')\n",
    "    })\n",
    "    reconstruct_mrc(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6dd1b-e678-4874-99ce-d3ed44155600",
   "metadata": {},
   "source": [
    "# Deep Finder experiments\n",
    "\n",
    "Train on 9 tomograms, eval on validation (last training) tomogram at every epoch. No early stopping. \\\n",
    "All jobs are written for SLURM submission system, except those that run within this notebook with multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fcff-a658-4dd9-bbcc-ea9b6ec057a8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c78ccc-cd0b-462d-96fc-d4b517bdb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SBATCH config will depend on your SLURM config, therefore will probably not run\n",
    "# without your manual adjustment. E.g. --qos names are fully custom.\n",
    "def get_full_DF_training_command(DF_training_command, config, with_header=True):\n",
    "    command_header = '' if not with_header else (\n",
    "        \"#!/bin/bash\\n\"\n",
    "        f\"#SBATCH --job-name={config['expname']}\\n\"\n",
    "        \"#SBATCH --qos=gpus4\\n\" # normal\n",
    "        \"#SBATCH --gres=gpu:1\\n\"\n",
    "        \"#SBATCH --cpus-per-task=10\\n\"\n",
    "        \"#SBATCH --nodes=1\\n\"\n",
    "        \"#SBATCH --ntasks=1\\n\"\n",
    "        \"#SBATCH --output=reproduce/training/%x.out\\n\\n\")\n",
    "\n",
    "    command = (\n",
    "        f\"EXPNAME={config['expname']} PYTHONHASHSEED=0 \"\n",
    "        f\"{DF_training_command} \"\n",
    "        f\"--training_tomogram_ids {' '.join(list(zip(*config['training_tomograms']))[0])} \"\n",
    "        f\"--training_tomograms {' '.join(list(zip(*config['training_tomograms']))[1])} \"\n",
    "        f\"{' '.join([f'{k} {v}' for k, v in config.items() if k.startswith('--')])} \"       \n",
    "    )\n",
    "    return command_header + command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa266a-6e18-44ef-997b-4944b527e5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training of Deep Finder for all the modalities.\n",
    "DF_training_command ='python3 faket/deepfinder/launch_training.py'\n",
    "folder = 'reproduce/training'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "jobs_per_gpu = 2\n",
    "# You can fit 2 of these jobs on the same A100 40GB GPU. \n",
    "# This way you can save ~40% of time as opposed to running 1 job per GPU.\n",
    "# 10 CPUs per task both tasks is enough (did not try less, but more does not help.)\n",
    "# Running 2 jobs in parallel (using 9 training tomograms) takes ~1060s to finish. \n",
    "# Running 2 jobs in parallel (using 1 training tomogram) takes ~120s to finish.\n",
    "\n",
    "jobs = []\n",
    "# modalities = ['rstyled']#, 'styled', 'noisy', 'baseline', 'shrec', 'noiseless', 'gauss']\n",
    "modalities = ['baseline', 'gauss', 'styled', 'noisy']\n",
    "n_tomos = range(9)\n",
    "num_seeds = 6\n",
    "num_epochs = 70\n",
    "\n",
    "for idf in modalities:\n",
    "    experiment_names = [f'exp_{idf}']\n",
    "    training_tomograms = [f'{idf}']\n",
    "\n",
    "    # command_queue = []\n",
    "    for experiment_name, training_tomogram in zip(experiment_names, training_tomograms):\n",
    "        for i, N in enumerate(range(1, num_seeds + 1)):\n",
    "            training_conf = {\n",
    "                \"expname\": f\"{experiment_name}_seed-{N:02d}\",\n",
    "                \"training_tomograms\": [[str(i), training_tomogram] for i in n_tomos],\n",
    "                \"--training_tomo_path\": data_folder,\n",
    "                \"--num_epochs\": num_epochs,\n",
    "                \"--out_path\": pj('data', 'results', experiment_name, f'seed{N}'),\n",
    "                \"--save_every\": 1, \n",
    "                \"--seed\": N,\n",
    "                # If continue_training_path is the same as out_path - continue from last epoch.\n",
    "                # If it is a path to a specific weights.h5 file, continue from there.\n",
    "                \"--continue_training_path\": pj('data', 'results', experiment_name, f'seed{N}'),\n",
    "\n",
    "            }\n",
    "            with_header = False if i % jobs_per_gpu else True\n",
    "            command = get_full_DF_training_command(DF_training_command, training_conf, with_header)\n",
    "            \n",
    "            # Add the slurm header only once per file\n",
    "            if with_header:\n",
    "                job_path = pj(folder, f\"{training_conf['expname']}.sbatch\")\n",
    "                mode = 'w'\n",
    "            else:\n",
    "                command = f'&\\n\\n' + command\n",
    "                mode = 'a'\n",
    "            \n",
    "            # Tell slurm to wait for all jobs to finish \n",
    "            if jobs_per_gpu > 1 and ((i+1) % jobs_per_gpu == 0):\n",
    "                command += '&\\n\\nwait < <(jobs -p)'  \n",
    "            \n",
    "            with open(job_path, mode) as fl:\n",
    "                fl.writelines(command) \n",
    "            if job_path not in jobs:\n",
    "                jobs.append(job_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147de33-18d6-4f84-8445-8c7d006e6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do my jobs finish approximately? Just an approx. helper function.\n",
    "# Based on timing on our hardware. There are decent chances your timing will be different.\n",
    "def duration_training(n_modalities, n_tomos, n_seeds, n_epochs, n_parallel, n_gpus):\n",
    "    from datetime import datetime, timedelta\n",
    "    assert n_parallel in [1, 2]\n",
    "    tomo_epoch = [None, 110, 125] # seconds    \n",
    "    duration_h = ((tomo_epoch[n_parallel] / 3600) * n_epochs * n_seeds \\\n",
    "                  * n_tomos * n_modalities) / n_parallel / n_gpus\n",
    "    print(f'Jobs will take ~{duration_h:.2f}h to finish.')\n",
    "    end = datetime.now() + timedelta(hours=duration_h)\n",
    "    print('From now that would be {:%d/%m/%Y %H:%M:%S}'.format(end))\n",
    "    \n",
    "# Do not forget to specify how many GPUs do you have available\n",
    "duration_training(len(modalities), len(n_tomos), num_seeds, num_epochs, jobs_per_gpu, n_gpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a1df6-5273-4553-8339-83fe0a70eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs to the SLURM queue\n",
    "for job_path in jobs:\n",
    "    !sbatch $job_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99fc5a-c6b3-4fe2-97b2-3f3891557c21",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "Wait until training jobs are computed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bf698-b2ae-44de-8574-265ec3913ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SBATCH config will depend on your SLURM config, therefore will probably not run\n",
    "# without your manual adjustment. E.g. --qos names are fully custom.\n",
    "def get_full_DF_analysis_command(DF_analysis_command, config, with_header=True):\n",
    "    command_header = '' if not with_header else (\n",
    "        \"#!/bin/bash\\n\"\n",
    "        f\"#SBATCH --job-name={config['expname']}\\n\"\n",
    "        \"#SBATCH --qos=gpus4\\n\"\n",
    "        \"#SBATCH --gres=gpu:1\\n\"\n",
    "        \"#SBATCH --cpus-per-task=10\\n\"\n",
    "        \"#SBATCH --nodes=1\\n\"\n",
    "        \"#SBATCH --ntasks=1\\n\"\n",
    "        \"#SBATCH --output=reproduce/segmentation/%x.out\\n\\n\")\n",
    "    \n",
    "    command=(\n",
    "        f\"EXPNAME={config['expname']} PYTHONHASHSEED=0 \"\n",
    "        f\"{DF_analysis_command} \"\n",
    "        f\"{' '.join([f'{k} {v}' for k, v in config.items() if k.startswith('--')])} \"\n",
    "    )\n",
    "    return command_header + command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537ca48-9d2b-429f-a4c8-a6dd97b4be2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running the segmentation for all modalities with baseline as a test tomogram\n",
    "\n",
    "DF_segmentation_command ='python3 faket/deepfinder/launch_segmentation.py'\n",
    "folder = 'reproduce/segmentation'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "jobs_per_gpu = 3\n",
    "# You can fit 3 of these jobs on the same A100 40GB GPU. \n",
    "# Segmenting 1 epoch checkpoint should take approx. 160s.\n",
    "\n",
    "jobs = []\n",
    "\n",
    "seed_ids = range(1, 7)\n",
    "num_epochs = range(1, 71, 1)\n",
    "modalities = {\n",
    "    'styled': {\n",
    "        'test_tomograms': ['shrec'],\n",
    "        'test_tomogram_indices': [9]\n",
    "    }, \n",
    "    'noisy': {\n",
    "        'test_tomograms': ['shrec'],\n",
    "        'test_tomogram_indices': [9]\n",
    "    }, \n",
    "    'baseline': {\n",
    "        'test_tomograms': ['shrec'],\n",
    "        'test_tomogram_indices': [9]\n",
    "    }, \n",
    "    'gauss': {\n",
    "        'test_tomograms': ['shrec'],\n",
    "        'test_tomogram_indices': [9]\n",
    "    },\n",
    "}\n",
    "\n",
    "i = 0\n",
    "segmentation_command_queue = []\n",
    "segmentation_job_paths = []\n",
    "segmentation_kwargs = []\n",
    "for modality, tomograms in modalities.items():\n",
    "    for test_tomo_modality in tomograms['test_tomograms']:\n",
    "        for test_tomo_index in tomograms['test_tomogram_indices']:\n",
    "            for N, epoch in product(seed_ids, num_epochs):\n",
    "                analysis_conf = {\n",
    "                    \"expname\": f'exp_{modality}_seed{N}_epoch{epoch:03d}_2021_model_{test_tomo_index}_{test_tomo_modality}',\n",
    "                    \"--test_tomo_path\" : data_folder,\n",
    "                    \"--test_tomo_idx\" : test_tomo_index, \n",
    "                    \"--test_tomogram\" : test_tomo_modality,\n",
    "                    \"--num_epochs\" : epoch,\n",
    "                    \"--DF_weights_path\" : pj('data', 'results', f'exp_{modality}', f'seed{N}'),\n",
    "                    \"--out_path\" : pj('data', 'results', f'exp_{modality}', f'seed{N}'), \n",
    "                    # \"--overwrite\"\n",
    "                }\n",
    "                \n",
    "                segmentation_kwargs.append(analysis_conf)\n",
    "                \n",
    "                with_header = False if i % jobs_per_gpu else True\n",
    "                command = get_full_DF_analysis_command(DF_segmentation_command, analysis_conf, with_header)\n",
    "                segmentation_command_queue.append(command)\n",
    "\n",
    "                # Add the slurm header only once per file\n",
    "                if with_header:\n",
    "                    job_path = pj(folder, f\"{analysis_conf['expname']}.sbatch\")\n",
    "                    mode = 'w'\n",
    "                else:\n",
    "                    command = f'&\\n\\n' + command\n",
    "                    mode = 'a'\n",
    "\n",
    "                # Tell slurm to wait for all jobs to finish \n",
    "                if jobs_per_gpu > 1 and ((i+1) % jobs_per_gpu == 0):\n",
    "                    command += '&\\n\\nwait < <(jobs -p)'  \n",
    "                    \n",
    "                segmentation_job_paths.append(job_path)\n",
    "\n",
    "                with open(job_path, mode) as fl:\n",
    "                    fl.writelines(command) \n",
    "                if job_path not in jobs:\n",
    "                    jobs.append(job_path)\n",
    "                \n",
    "                i+=1 # job counter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2fb51-a708-457c-8a70-49b38f7b8703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submit jobs to the SLURM queue\n",
    "for job_path in jobs:\n",
    "    !sbatch $job_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc53cf7-c55d-47fe-9da3-620832d0df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking if all was computed\n",
    "missing = set()\n",
    "for kwargs, jobpath in zip(segmentation_kwargs, segmentation_job_paths):\n",
    "    out_path = kwargs['--out_path']\n",
    "    tidx = kwargs['--test_tomo_idx']\n",
    "    tt = kwargs['--test_tomogram']\n",
    "    epochs = kwargs['--num_epochs']\n",
    "    p = f'{out_path}/epoch{epochs:03d}_2021_model_{tidx}_{tt}_bin2_labelmap.mrc'\n",
    "    if not os.path.exists(p):\n",
    "        missing.add(jobpath)\n",
    "\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9eb65-f055-431a-a7b4-0b9b262a46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs that were not computed to the SLURM queue\n",
    "for job_path in missing:\n",
    "    !sbatch $job_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811eb7f0-b743-4614-b5c9-63e1569c3cdd",
   "metadata": {},
   "source": [
    "## Clustering & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a51e7-b337-4962-924e-92582263e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running the evaluation, make sure that `particle_locations.txt` file is present in the test tomogram folder.\n",
    "for model in ['model_8', 'model_9']:\n",
    "    src = pj(data_folder, model, 'particle_locations.txt')\n",
    "    dst = pj(data_folder, model, 'faket', 'particle_locations.txt')\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398461e2-c16f-4ec7-9f00-71f6f582c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SBATCH config will depend on your SLURM config, therefore will probably not run\n",
    "# without your manual adjustment. E.g. --qos names are fully custom.\n",
    "\n",
    "# Template of the sbatch file that runs commands from each line of the <folder>/command.queue file\n",
    "sbatch_template = '''#!/bin/bash\n",
    "#SBATCH --job-name={jobname}\n",
    "#SBATCH --qos=cpus150\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --array=1-{njobs}  # Must start from 1 until end (including) so 1-3 runs indices 0, 1, 2\n",
    "#SBATCH --open-mode=append\n",
    "#SBATCH --output={folder}/%x.out\n",
    "\n",
    "sed -n ${slurm_task_id_var}p {folder}/{jobname}.queue | bash\n",
    "'''\n",
    "\n",
    "# Clustering jobs\n",
    "DF_clustering_command ='python3 -m faket.deepfinder.launch_clustering'\n",
    "folder_clustering = 'reproduce/clustering'\n",
    "command_queue_clustering = []\n",
    "os.makedirs(folder_clustering, exist_ok=True)\n",
    "\n",
    "# Evaluation jobs\n",
    "DF_evaluation_command ='python3 faket/deepfinder/launch_evaluation.py'\n",
    "folder_evaluation = 'reproduce/evaluation'\n",
    "command_queue_evaluation = []\n",
    "os.makedirs(folder_evaluation, exist_ok=True)\n",
    "\n",
    "# Storing kwargs if we want to run the jobs with multiprocessing instead of SLURM\n",
    "kwargs_clustering = []\n",
    "\n",
    "# Here we take the same seed_ids, num_epochs, modalities from the segmentation task.\n",
    "for modality, tomograms in modalities.items():\n",
    "    for test_tomo_modality in tomograms['test_tomograms']:\n",
    "        for test_tomo_index in tomograms['test_tomogram_indices']:\n",
    "            for N, epoch in product(seed_ids, num_epochs):\n",
    "                analysis_conf = {\n",
    "                    \"expname\": f'exp_{modality}_seed{N}_epoch{epoch:03d}_2021_model_{test_tomo_index}_{test_tomo_modality}',\n",
    "                    # kwargs must stay in this order otherwise multiprocessing starmap will make problems\n",
    "                    \"--test_tomogram\" : test_tomo_modality,\n",
    "                    \"--test_tomo_idx\" : test_tomo_index, \n",
    "                    \"--num_epochs\" : epoch,\n",
    "                    \"--label_map_path\" : pj('data', 'results', f'exp_{modality}', f'seed{N}'),\n",
    "                    \"--out_path\" : pj('data', 'results', f'exp_{modality}', f'seed{N}'), \n",
    "                    \"--n_jobs\": 1,  # Always use 1 since we will later use multiprocessing\n",
    "                    # \"--overwrite\": True, # Specify true only if running with multiprocessing, otherwise ''\n",
    "                    # \"--only_apply_thresholding\": True, # Specify true only if running with multiprocessing, otherwise ''\n",
    "                    # clustering also accepts n_jobs, overwrite, only_apply_thresholding args\n",
    "                }\n",
    "                \n",
    "                command_clustering = get_full_DF_analysis_command(DF_clustering_command, analysis_conf, with_header=False)\n",
    "                command_queue_clustering.append(command_clustering)\n",
    "                \n",
    "                eval_conf = {k: v for k, v in analysis_conf.items() if k in [\"expname\", \"--test_tomogram\", \"--test_tomo_idx\", \"--num_epochs\", \"--label_map_path\", \"--out_path\"]}\n",
    "                command_evaluation = get_full_DF_analysis_command(DF_evaluation_command, eval_conf, with_header=False)\n",
    "                command_queue_evaluation.append(command_evaluation)\n",
    "                \n",
    "                del analysis_conf['expname']\n",
    "                kwargs_clustering.append(analysis_conf)\n",
    "        \n",
    "# Save the command queue to a file and create the associated sbatch file \n",
    "jobname = 'clustering_baseline'\n",
    "clustering_filename = pj(folder_clustering, jobname)\n",
    "with open(f'{clustering_filename}.queue', 'w') as fl:\n",
    "    fl.writelines('\\n'.join(command_queue_clustering))\n",
    "with open(f'{clustering_filename}.sbatch', 'w') as fl:\n",
    "    fl.writelines(sbatch_template.format(jobname=jobname, njobs=len(command_queue_clustering), \n",
    "                                         folder=folder_clustering, slurm_task_id_var='{SLURM_ARRAY_TASK_ID}'))\n",
    "\n",
    "# Save the command queue to a file and create the associated sbatch file \n",
    "jobname = 'evaluation_baseline'\n",
    "evaluation_filename = pj(folder_evaluation, jobname)\n",
    "with open(f'{evaluation_filename}.queue', 'w') as fl:\n",
    "    fl.writelines('\\n'.join(command_queue_evaluation))\n",
    "with open(f'{evaluation_filename}.sbatch', 'w') as fl:\n",
    "    fl.writelines(sbatch_template.format(jobname=jobname, njobs=len(command_queue_evaluation), \n",
    "                                         folder=folder_evaluation, slurm_task_id_var='{SLURM_ARRAY_TASK_ID}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62f9a7-c0a9-4ec0-96c7-c807f5bc2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit clustering jobs to SLURM\n",
    "!sbatch $clustering_filename\\.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d7da5-9c7b-428a-b17c-4ddc88d26e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit evaluation jobs to SLURM\n",
    "!sbatch $evaluation_filename\\.sbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9aa501-d97a-48df-89b5-dfca36f8334c",
   "metadata": {},
   "source": [
    "**All the results should now be stored in the `data/results` folder.** \\\n",
    "**Use the `figures.ipynb` to further visualize the results.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
